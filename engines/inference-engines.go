package engines

import (
	"time"
)

/*

Description is generated by bloop.ai from vllm source code:

JSON exmaple:

{
  "prompts": ["San Francisco is a", "New York is a"],
  "n": 1,
  "stream": false,
  "log_level": "info",
  "logprobs": 10,
  "echo": false,
  "max_tokens": 16,
  "temperature": 0.0,
  "top_p": 1.0,
  "presence_penalty": 0.0,
  "frequency_penalty": 0.0,
  "best_of": 1,
  "stop_sequences": ["\n"]
}

In this example:

"prompts" is a list of texts you want the model to continue.
"n" is the number of completions to generate for each prompt.
"stream" is a boolean indicating whether to stream the results or not.
"log_level" is the logging level.
"logprobs" is the number of most probable tokens to log for each token.
"echo" is a boolean indicating whether to include the prompt in the response.
"max_tokens" is the maximum number of tokens in the generated text.
"temperature" controls the randomness of the model's output. A higher value (closer to 1) makes the output more random, while a lower value (closer to 0) makes it more deterministic.
"top_p" is used for nucleus sampling and controls the cumulative probability cutoff.
"presence_penalty" and "frequency_penalty" are advanced parameters that control the model's output.
"best_of" is the number of times to run the model and keep the best result.
"stop_sequences" is a list of sequences where the API will stop generating further tokens.
Please note that the actual parameters may vary depending on the specific implementation of the vLLM engine.

*/

const InferenceTimeout = 600 * time.Second

type RemoteInferenceEngine struct {
	EndpointUrl           string
	EmbeddingsEndpointUrl string
	MaxBatchSize          int
	Performance           float32 // tokens per second
	MaxRequests           int
	Models                []string // supported models
	RequestsServed        uint64
	TimeConsumed          time.Duration
	TokensProcessed       uint64
	TokensGenerated       uint64
	PromptTokens          uint64
	LeasedAt              time.Time
	Busy                  bool
	EmbeddingsDims        *uint64
	CompletionFailed      bool
	EmbeddingsFailed      bool
}

func StartInferenceEngine(engine *RemoteInferenceEngine, done chan struct{}) {
	// we need to send a completion request to the engine
	// detect the model, then send embeddings request to the engine and
	// detect the model and dimensions
	_, err := RunCompletionRequest(engine, []*JobQueueTask{
		{
			Req: &GenerationSettings{RawPrompt: "### Instruction\nProvide an answer. 2 + 2 = ?\n### Assistant: ", MaxRetries: 1, Temperature: 0.1},
		},
	})
	if err != nil {
		// engine failed to run completion
		engine.CompletionFailed = true
	}

	cEmb, err := RunEmbeddingsRequest(engine, []*JobQueueTask{
		{
			Req: &GenerationSettings{RawPrompt: "Hello world", MaxRetries: 1, Temperature: 0.1},
		},
	})
	if err != nil {
		// engine failed to run embeddings
		engine.EmbeddingsFailed = true
	}

	if len(cEmb) > 0 {
		if cEmb[0].Model != nil {
			added := false
			for idx, model := range engine.Models {
				if model == *cEmb[0].Model || model == "" {
					added = true
					engine.Models[idx] = *cEmb[0].Model
					break
				}
			}
			if !added {
				engine.Models = append(engine.Models, *cEmb[0].Model)
			}
		}
		if len(cEmb[0].VecF64) > 0 {
			dims := uint64(len(cEmb[0].VecF64))
			engine.EmbeddingsDims = &dims
		}
	}

	done <- struct{}{}
}
