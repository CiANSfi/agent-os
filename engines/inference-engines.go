package engines

import (
	"fmt"
	"github.com/logrusorgru/aurora"
	"github.com/olekukonko/tablewriter"
	"os"
	"time"
)

/*

Description is generated by bloop.ai from vllm source code:

JSON exmaple:

{
  "prompts": ["San Francisco is a", "New York is a"],
  "n": 1,
  "stream": false,
  "log_level": "info",
  "logprobs": 10,
  "echo": false,
  "max_tokens": 16,
  "temperature": 0.0,
  "top_p": 1.0,
  "presence_penalty": 0.0,
  "frequency_penalty": 0.0,
  "best_of": 1,
  "stop_sequences": ["\n"]
}

In this example:

"prompts" is a list of texts you want the model to continue.
"n" is the number of completions to generate for each prompt.
"stream" is a boolean indicating whether to stream the results or not.
"log_level" is the logging level.
"logprobs" is the number of most probable tokens to log for each token.
"echo" is a boolean indicating whether to include the prompt in the response.
"max_tokens" is the maximum number of tokens in the generated text.
"temperature" controls the randomness of the model's output. A higher value (closer to 1) makes the output more random, while a lower value (closer to 0) makes it more deterministic.
"top_p" is used for nucleus sampling and controls the cumulative probability cutoff.
"presence_penalty" and "frequency_penalty" are advanced parameters that control the model's output.
"best_of" is the number of times to run the model and keep the best result.
"stop_sequences" is a list of sequences where the API will stop generating further tokens.
Please note that the actual parameters may vary depending on the specific implementation of the vLLM engine.

*/

type EndpointProtocol int

const (
	EP_OpenAI = iota
	EP_VLLM
	EP_Custom
)

const A6000vLLMBatchSize = 128
const InferenceTimeout = 600 * time.Second

type InferenceEngine struct {
	EndpointUrl           string
	EmbeddingsEndpointUrl string
	Protocol              EndpointProtocol
	MaxBatchSize          int
	Performance           float32 // tokens per second
	MaxRequests           int
	Models                []string // supported models
	RequestsServed        uint64
	TimeConsumed          time.Duration
	TokensProcessed       uint64
	TokensGenerated       uint64
	PromptTokens          uint64
	LeasedAt              time.Time
	Busy                  bool
	EmbeddingsDims        *uint64
}

var InferenceEngines []*InferenceEngine

func GetInferenceEngines() []*InferenceEngine {
	return InferenceEngines
}

func init() {
	localLLM := &InferenceEngine{
		EndpointUrl:           "http://localhost:8000/v1/completions",
		EmbeddingsEndpointUrl: "http://127.0.0.1:8000/v1/embeddings",
		Protocol:              EP_OpenAI,
		MaxBatchSize:          1,
		Performance:           0,
		MaxRequests:           1,
		Models:                []string{""},
	}

	remoteVLLM1 := &InferenceEngine{
		EndpointUrl:  "http://127.0.0.1:8001/v1/completions",
		Protocol:     EP_OpenAI,
		MaxBatchSize: A6000vLLMBatchSize,
		Performance:  0,
		MaxRequests:  1,
		Models:       []string{""},
	}

	remoteVLLM2 := &InferenceEngine{
		EndpointUrl:  "http://127.0.0.1:8002/v1/completions",
		Protocol:     EP_OpenAI,
		MaxBatchSize: A6000vLLMBatchSize,
		Performance:  0,
		MaxRequests:  1,
		Models:       []string{""},
	}
	remoteVLLM3 := &InferenceEngine{
		EndpointUrl:  "http://127.0.0.1:8003/v1/completions",
		Protocol:     EP_OpenAI,
		MaxBatchSize: A6000vLLMBatchSize,
		Performance:  0,
		MaxRequests:  1,
		Models:       []string{""},
	}
	remoteVLLM4 := &InferenceEngine{
		EndpointUrl:  "http://127.0.0.1:8004/v1/completions",
		Protocol:     EP_OpenAI,
		MaxBatchSize: A6000vLLMBatchSize,
		Performance:  0,
		MaxRequests:  1,
		Models:       []string{""},
	}

	fmt.Printf("localLLM: %v\n", localLLM)
	InferenceEngines = []*InferenceEngine{
		localLLM,
		remoteVLLM1,
		remoteVLLM2,
		remoteVLLM3,
		remoteVLLM4,
	}

	// inferenceEngines = []*InferenceEngine{inferenceEngines[1]}
}

func StartInferenceEngines() {
	doneChannel := make(chan struct{}, 1024)
	for _, engine := range InferenceEngines {
		startInferenceEngine(engine, doneChannel)
	}

	for range InferenceEngines {
		<-doneChannel
	}

	tw := tablewriter.NewWriter(os.Stdout)
	tw.SetHeader([]string{"Endpoint", "MaxBatchSize", "MaxRequests", "Models", "Ok", "EmbeddingsDims"})
	tableData := make([][]string, 0)
	for _, engine := range InferenceEngines {
		tableData = append(tableData, []string{
			engine.EndpointUrl,
			fmt.Sprintf("%d", engine.MaxBatchSize),
			fmt.Sprintf("%d", engine.MaxRequests),
			fmt.Sprintf("%v", takeLastNRunes(engine.Models[0], 75)),
			fmt.Sprintf("%s", aurora.BrightGreen("YES")),
			fmt.Sprintf("%v", toUint64(engine.EmbeddingsDims)),
		})
	}
	tw.AppendBulk(tableData)
	tw.Render()
}

func toUint64(dims *uint64) string {
	if dims == nil {
		return "N/A"
	}
	return fmt.Sprintf("%d", *dims)
}

func takeLastNRunes(s string, n int) string {
	if len(s) <= n {
		return s
	}
	return s[len(s)-n:]
}

func startInferenceEngine(engine *InferenceEngine, done chan struct{}) {
	// we need to send a completion request to the engine
	// detect the model, then send embeddings request to the engine and
	// detect the model and dimensions
	_, err := RunCompletionRequest(engine, []*JobQueueTask{
		{
			Req: &GenerationSettings{RawPrompt: "Hello world", MaxRetries: 1},
		},
	})
	if err != nil {
		// engine failed to run completion
	}

	cEmb, err := RunEmbeddingsRequest(engine, []*JobQueueTask{
		{
			Req: &GenerationSettings{RawPrompt: "Hello world", MaxRetries: 1},
		},
	})

	if err != nil {
		// engine failed to run embeddings
	}

	if len(cEmb) > 0 {
		if cEmb[0].Model != nil {
			added := false
			for idx, model := range engine.Models {
				if model == *cEmb[0].Model || model == "" {
					added = true
					engine.Models[idx] = *cEmb[0].Model
					break
				}
			}
			if !added {
				engine.Models = append(engine.Models, *cEmb[0].Model)
			}
		}
		if len(cEmb[0].VecF64) > 0 {
			dims := uint64(len(cEmb[0].VecF64))
			engine.EmbeddingsDims = &dims
		}
	}

	done <- struct{}{}
}
